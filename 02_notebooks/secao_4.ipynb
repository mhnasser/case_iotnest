{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f048d1-a775-4493-b26f-53cf527c03f3",
   "metadata": {},
   "source": [
    "# 1. CI/CD para Modelos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba917a-ade6-4989-b3af-58b42eb26bdc",
   "metadata": {},
   "source": [
    "### O treinamento de um modelo de machine learning\n",
    "\n",
    "Antes de qualquer escolha de modelo de ML, é primordial entender o problema de negócio junto aos usuários. A implementação de um algoritmo consiste em horas de desenvolvimento, testes, retreinamento e monitoramento, o que pode ser oneroso se o caso não for bem estudado de início.\n",
    "\n",
    "Considerando que o business case foi avaliado propriamente e a implementação de um modelo de clustering corresponde às demandas de negócio. Usando o framework `sklearn`, o passo-a-passo, no meu ponto de vista, seria:\n",
    "\n",
    "1. **Ter um dataset de treino preparado**. Se for uma lista de dicionários, por exemplo, será necessário convertê-lo em um `numpy.ndarray` para usarmos na entrada do algoritmo de clustering.\n",
    "\n",
    "2. **Definir os hiperparâmetros** específicos do algoritmo de ML escolhido.  \n",
    "   Como escolhemos clustering, será necessário estudar a quantidade ideal de grupos. Para isso podemos utilizar técnicas como o *Elbow Method*, por exemplo. Dessa forma, teremos nosso número de clusters.\n",
    "\n",
    "3. **Adicionar regras de negócio ao modelo**, como delimitação do raio de cada centróide do cluster e afins.\n",
    "\n",
    "4. **Visualizar o resultado do modelo**. Após o treinamento, o cientista pode plotar o resultado do modelo junto aos seus clusters em um mapa utilizando frameworks como `Folium`. Seria interessante, pois a visualização do modelo e clusters via `Folium` pode ser personalizada com cores, tornando-se entendível até para os usuários.\n",
    "\n",
    "5. **Salvar o modelo e registrar os artefatos do experimento (ainda em DEV)**.  \n",
    "   Esta etapa é importante, pois o modelo será usado futuramente no pipeline de inferência. Plataformas open-source como `MLflow` já implementam práticas e tarefas de registro e deployment de modelos de machine learning. No entanto, a depender da empresa, outras plataformas podem realizar o registro dos artefatos. O `Databricks` se destaca. A AWS também possui esse serviço. Podemos salvar os artefatos manualmente da mesma forma, principalmente quando não é necessário criar modelos de machine learning em massa.\n",
    "\n",
    "Dessa forma, criamos o modelo, treinamos, salvamos e registramos o experimento e seus artefatos utilizando o `MLflow`, etapa condizente para eventuais auditorias e fallback.\n",
    "\n",
    "Este pipeline de treino pode ser automatizado via script. O script, salvo no diretório principal do projeto, executa os dados de treino, artefatos e todo o processo descrito acima, salvando o nome do experimento no `MLflow`, por exemplo.\n",
    "\n",
    "## A implantação em produção\n",
    "\n",
    "Usando ferramentas como `Docker`, `Kubernetes` ou plataformas em nuvem, é possível colocar o modelo em produção. É preciso ficar claro que não existe uma única forma de prosseguir com o processo de produção da modelagem após a análise exploratória do projeto. Existem dois cenários muito comuns que podemos escolher:\n",
    "\n",
    "1. **Deployment manual do modelo**, criando uma API (do zero) e \"deployando\" em um ambiente escalável (`Kubernetes`).\n",
    "2. **Deployment do modelo por meio de plataformas que facilitam MLOps**, utilizando modelos de *model serving*.\n",
    "\n",
    "Abaixo, temos um design de referência para o pipeline de inferência (*online serving*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f075504-4f39-42aa-807b-302408c3595d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"00_images/Imagem1.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b494f94-0b12-4144-a8ad-e693a33fe443",
   "metadata": {},
   "source": [
    "# Simulando um processo de CI/CD do GitHub Actions no localhost\n",
    "\n",
    "Vamos simular um processo de CI/CD do GitHub Actions no localhost de forma open-source. Iremos chamar o modelo através de uma API. \n",
    "\n",
    "## Passo 1: Gerar a versão da API\n",
    "\n",
    "Esta API deve ser desenvolvida com todos os parâmetros `path` correspondendo à arquitetura levantada na questão 1. Após o desenvolvimento, deve-se:\n",
    "\n",
    "1. Criar um novo repositório no GitHub (por ser um projeto open-source, mas o Bitbucket também poderia ser usado).\n",
    "2. Adicionar o código da API na branch `main`.\n",
    "\n",
    "## Deploy da API\n",
    "\n",
    "Para o deploy da API, as opções incluem:\n",
    "\n",
    "1. **Docker com Kubernetes**: Ideal para maior escalabilidade.\n",
    "2. **Render**: Uma plataforma mais barata e amigável para hospedar aplicações.\n",
    "\n",
    "### Deploy da API no Render\n",
    "\n",
    "A forma mais simples para fazer o deploy da API no Render é utilizando o seguinte template:  \n",
    "[Template Render FastAPI](https://github.com/new?template_name=fastapi&template_owner=render-examples).  \n",
    "\n",
    "1. Escolha o repositório GitHub da sua API.  \n",
    "2. Defina o seguinte comando para iniciar sua aplicação:  \n",
    "\n",
    "    ```bash\n",
    "   puvicorn main_render:app --host 0.0.0.0 --port $PORT \n",
    "\n",
    "Importante salientar que, se procuramos escalabilidade e disponibilidade, o Render pode não ser a opção ideal, sendo o kubernetes mais apropriado para a tarefa, a depender do budget do projeto. Neste caso, podemos copiar o modelo para o bucket no S3 para maior versatilidade de ferramentas hospedagem, visto que o MLflow tem problemas de escalabilidade, apesar de fornecer recursos de model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05355cb0-1526-49eb-ada7-880ae6ddfa2c",
   "metadata": {},
   "source": [
    "# 2. CI/CD para Modelos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823cdba-4be6-409f-9e3b-e94a3d066806",
   "metadata": {},
   "source": [
    "O código para aplicação está todo na pasta 03_scripts. Abaixo pode conferir a API funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e31e58a-a6d9-4768-9bb8-441706b212af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c219bba1-da3e-4608-a40d-1e4d9007726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\t<html>\n",
      "\t  <head>\n",
      "\t\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "\t\t<meta charset=\"utf-8\">\n",
      "\t\t<title>Application Error</title>\n",
      "\t\t<style media=\"screen\">\n",
      "\t\t  html,body,iframe {\n",
      "\t\t\tmargin: 0;\n",
      "\t\t\tpadding: 0;\n",
      "\t\t  }\n",
      "\t\t  html,body {\n",
      "\t\t\theight: 100%;\n",
      "\t\t\toverflow: hidden;\n",
      "\t\t  }\n",
      "\t\t  iframe {\n",
      "\t\t\twidth: 100%;\n",
      "\t\t\theight: 100%;\n",
      "\t\t\tborder: 0;\n",
      "\t\t  }\n",
      "\t\t</style>\n",
      "\t  </head>\n",
      "\t  <body>\n",
      "\t\t<iframe src=\"//www.herokucdn.com/error-pages/application-error.html\"></iframe>\n",
      "\t  </body>\n",
      "\t</html>\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://iotnest-api-case-3cbc3428a98c.herokuapp.com/health\")\n",
    "print(response.text)  # Saída esperada: \"Serviço em execução\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd3ce92a-4258-4211-a91f-cd69127c8bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previsões: {'predictions': [0, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de entradas x e y\n",
    "x = np.array([0.82737724, -0.73769513, -0.44605037, 0.43279337, -0.47367361,\n",
    "              -0.50244517, 0.58595414, 0.05915988, -0.09502409])\n",
    "\n",
    "y = np.array([-1.56610693, 1.35557354, 0.71503732, 0.43279337, -0.47367361,\n",
    "              0.78684529, -1.9423032, 0.05915988, 1.16865443])\n",
    "\n",
    "# Convertendo as entradas para listas\n",
    "data = {\n",
    "    \"inputs\": [x.tolist(), y.tolist()]  # Enviando múltiplas entradas\n",
    "}\n",
    "\n",
    "url = \"http://127.0.0.1:5000/predict\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    print(\"Previsões:\", response.json())\n",
    "else:\n",
    "    print(f\"Erro: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27383639-67d0-4d7e-99b3-911a3da21ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'numpy' nÆo ‚ reconhecido como um comando interno\n",
      "ou externo, um programa oper vel ou um arquivo em lotes.\n"
     ]
    }
   ],
   "source": [
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0f57a-b213-4fbb-bf24-2d11803fb907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
